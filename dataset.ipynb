{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import CustomImageDataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauli\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\pauli\\OneDrive - University of Surrey\\work\\emotion-recognition\\dataset.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data = CustomImageDataset(dataset_path=\"../Datasets/face-emotion\", mode=\"train\", transform=\"scale_224\")\n",
    "test_data = CustomImageDataset(dataset_path=\"../Datasets/face-emotion\", mode=\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=True)\n",
    "\n",
    "train_features, train_labels = training_data.__getitem__(120)\n",
    "\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example string labels\n",
    "labels = [\"happy\", \"sad\"]\n",
    "\n",
    "# Convert string labels to numerical labels\n",
    "label_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n",
    "numerical_labels = torch.tensor([label_to_idx[label] for label in labels])\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "num_classes = len(label_to_idx)\n",
    "one_hot_labels = torch.nn.functional.one_hot(numerical_labels, num_classes)\n",
    "\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
